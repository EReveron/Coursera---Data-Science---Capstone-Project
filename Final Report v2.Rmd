---
title: "Capstone Project - Final Report"
author: "Enrique Reveron"
date: "July 14, 2016"
output:
  html_document:
    fig_height: 4
    fig_width: 9
    keep_md: yes
    theme: default
  pdf_document: default
  word_document: default
---


```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=TRUE)
```

## Executive Summary

This is a Milestone report related with the Coursera Capstone Project, the target is show initial exploratory data analysis about the US dataset that include three kind of files:

* Twitter
* News
* Blogs


## 1. Optimizations 

$$\sum_{i=1}^n X_i$$

$$\alpha, \beta,  \gamma, \Gamma$$

$$a \pm b$$
$$x \ge 15$$
$$a_i \ge 0~~~\forall i$$

$$\sum_{i=1}^{n}\left( \frac{X_i}{Y_i} \right)$$

$$\sum_{i=1}^{n} X^3_i$$

### 1.1 Create Ngrams Tables

###	1.1.1 quanteda Library

#####		1.1.1.1 no corpus

####    1.1.1.2 Strategy to clean the corpora ( EEE for punctuations)

####		1.1.1.3 tokenizer + remove bad words before dfm
		
####		1.1.1.4 tokenizer unigrams + to create tokenizer of ngrams
				
####		1.1.1.5 dfm + dfm clean

### 1.1.2 data frame vs data table

### 1.2 Method to Calculate Probabilities of Words to make Prediction

#### 1.2.1 MLE

#### 1.2.2 Knersey-ney

#### 1.2.2.1 Recursive

#### 1.2.2.2 Recursive + saving important values

#### 1.2.2.3 Iterative whithout joins

#### 1.2.2.4 Iterative + joins

### 1.3 Predict Next Words

#### 1.3.1 Backoff Implementation

#### 1.3.2 Complete words

#### 1.3.3 Incomplete words

#### 1.3.4 Number of words to predict

#### 1.3.5 How to reduce the number of words / Minimun Probability to Consider

### 1.4 Shiny App 

### 1.4.1 Only use the nth most important words (higher probability)

### 1.4.2 Remove which words?

### 1.4.3 App Parameters

#### 1.4.3.1 Number of Words to Predict

#### 1.4.3.2 Minimun Probability to Consider

#### 1.4.3.3 Predict next words considering Complete / Incomplete Words

#### 1.4.3.4 Choose the Ngrams (20%,40%,60%,80%) of the total




## 1. Load the Neccesary Libraries

For this project we will use basicly the **quanteda**,**ggplot2**, **knitr** and **RColorBrewer**.

```{r init , echo=TRUE}
library(quanteda)
library(data.table)
library(ggplot2)
library(knitr)

#wd.R <- "D:/Coursera/Capstone Project/Coursera---Data-Science---Capstone-Project"
wd.R <- "D:/001 -- Coursera/Capstone Project/Coursera---Data-Science---Capstone-Project"

#wd.RData <- "D:/Coursera/Capstone Project/Coursera-SwiftKey/final/en_US/"
wd.RData <- "D:/001 -- Coursera/Capstone Project/Coursera-SwiftKey/final/en_US"

# To load the R local files
setwd(wd.R)

source("Create Ngrams Data Table vFinal 2.R")
source("Knersey-ney Optimazed vFinal 5.R")
source("Main Predict Word vFinal.R")
source("Pred Next Word Regex vFinal.R")
source("Pred Next Word vFinal.R")

# For reproducibility
set.seed(12345)

```

## 2. Create Ngram Data Table

In order to create the Ngrams we will do several steps

### 2.1 Load All the Data

The files that we will use in the project are bigger than **150Mbytes** each one. In order to do the exploratory data analysis and to have an acceptable runtime, I will use only **10%** of the data. 

```{r createdata,echo=TRUE, cache = TRUE}
list_filenames <- c("en_US.blogs.txt","en_US.news.txt","en_US.twitter.txt")
create_mydata(list_filenames, 80) 
```

### 2.2 Create All Tokens

```{r createalltokens,echo=TRUE, cache = TRUE}
create_alltokens(list_files,80)
```


### 2.3 Create Ngrams 

#### 2.3.1 Create and Clean Unigrams

```{r create_unigram, echo=TRUE, cache = TRUE}
create_ngram(n=1,list_filenames,training_set= 80) 
```


```{r clean_unigram, echo=TRUE, cache = TRUE}
clean_ngram(n=1,list_filenames,training_set= 80) 
```



#### 2.3.2 Create and Clean Bigrams

```{r create_bigram, echo=TRUE, cache = TRUE}
create_ngram(n=2,list_filenames,training_set= 80)
```



```{r clean_bigram, echo=TRUE, cache = TRUE}
clean_ngram(n=2,list_filenames,training_set= 80)
```


#### 2.3.3 Create and Clean Trigrams

```{r create_trigram,echo=TRUE, cache = TRUE}
create_ngram(n=3,list_filenames,training_set= 80)
```


```{r clean_trigram,echo=TRUE, cache = TRUE}
clean_ngram(n=3,list_filenames,training_set= 80)
```


#### 2.3.4 Create and Clean Quadgrams

```{r create_quadgram,echo=TRUE, cache = TRUE}
create_ngram(n=4,list_filenames,training_set= 80)
```


```{r clean_quadgram,echo=TRUE, cache = TRUE}
clean_ngram(n=4,list_filenames,training_set= 80)
```


### 2.4 Create and Trim DFM 

#### 2.4.1 Create and Trim Uni-dfm

```{r create_unidfm,echo=TRUE, cache = TRUE}
create_dfm(n=1,list_filenames,training_set= 80)
```


```{r trim_unidfm,echo=TRUE, cache = TRUE}
trim_dfm(n=1,list_filenames,training_set= 80,mincount=1)
```


#### 2.4.2 Create and Trim Bi-dfm

```{r create_bidfm,echo=TRUE, cache = TRUE}
create_dfm(n=2,list_filenames,training_set= 80)
```


```{r clean_unidfm,echo=TRUE, cache = TRUE}
trim_dfm(n=2,list_filenames,training_set= 80,mincount=1)
```


#### 2.4.3 Create and Trim Tri-dfm

```{r create_tridfm,echo=TRUE, cache = TRUE}
create_dfm(n=3,list_filenames,training_set= 80)
```


```{r trim_tridfm,echo=TRUE, cache = TRUE}
trim_dfm(n=3,list_filenames,training_set= 80,mincount=1)
```

#### 2.4.4 Create and Trim Quad-dfm

```{r create_quaddfm,echo=TRUE, cache = TRUE}
create_dfm(n=4,list_filenames,training_set= 80)
```


```{r trim_quaddfm,echo=TRUE, cache = TRUE}
trim_dfm(n=4,list_filenames,training_set= 80,mincount=1)
```


### 2.5 Create Data Table with Tokens and Frequency

#### 2.5.1 Create Unigram Data Table with Tokens and Frequency

```{r create_uniDT,echo=TRUE, cache = TRUE}
create_DT(n=1,list_filenames,training_set= 80,mincount=1)
```


#### 2.5.2 Create Bigram Data Table with Tokens and Frequency

```{r create_biDT,echo=TRUE, cache = TRUE}
create_DT(n=2,list_filenames,training_set= 80,mincount=1)
```

#### 2.5.3 Create Trigram Data Table with Tokens and Frequency

```{r create_triDT,echo=TRUE, cache = TRUE}
create_DT(n=3,list_filenames,training_set= 80,mincount=1)
```

#### 2.5.4 Create Quadgram Data Table with Tokens and Frequency

```{r create_quadDT,echo=TRUE, cache = TRUE}
create_DT(n=4,list_filenames,training_set= 80,mincount=1)
```

### 3. Calculate Probability for Each Ngram

### 3.1 Create Unigram Knersey-ney Probability Table

The files that we will use in the project are bigger than **150Mbytes** each one. In order to


```{r calculate_uniProb, echo=TRUE}
calculate_prob_kn(n=1,training_set=80,p1=1)
```


Those are information in the Unigram DT:

```{r echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(DT.uni[1:10])
```

The prob table has:

```{r echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(DT.uni.prob[1:10])
```

### 3.2 Create Bigram Knersey-ney Probability Table

The files that we will use in the project are bigger than **150Mbytes** each one. In order to


```{r calculate_biProb,echo=TRUE, cache = TRUE}
calculate_prob_kn(n=2,training_set=80,p1=1)
```

Those are information in the Unigram DT:

```{r echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(DT.bi[1:10])
```

The prob table has:

```{r echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(DT.bi.prob[1:10])
```


### 3.3 Create Trigram Knersey-ney Probability Table

The files that we will use in the project are bigger than **150Mbytes** each one. In order to


```{r calculate_triProb,echo=TRUE, cache = TRUE}
calculate_prob_kn(n=3,training_set=80,p1=1)
```

Those are information in the Unigram DT:

```{r echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(DT.tri[1:10])
```

The prob table has:

```{r echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(DT.tri.prob[1:10])
```

### 3.4 Create Quadgram Knersey-ney Probability Table

The files that we will use in the project are bigger than **150Mbytes** each one. In order to


```{r calculate_quadProb,echo=TRUE, cache = TRUE}
calculate_prob_kn(n=4,training_set=80,p1=1)
```

Those are information in the Unigram DT:

```{r echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(DT.quad[1:10])
```

The prob table has:

```{r echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(DT.quad.prob[1:10])
```


## 4. Ngrams Probability Frequency Table Analysis

```{r dt_prob_freq,echo=TRUE, cache = TRUE}
DT.prob.freq <- DT_prob_freq(training_set=80) 
```


```{r echo=TRUE, results='asis'}
kable(DT.prob.freq)
```

## 5. Singleton Probability Table

```{r dt_prob_sing,echo=TRUE, cache = TRUE}
DT.prob.sing <- DT_prob_singletons(training_set=80) 
```



```{r echo=TRUE, results='asis'}
kable(DT.prob.sing)
```

## 6. Prediction of next word

### 6.1 Example with Unigram

```{r predic_uni1, echo=TRUE} 
#source("Pred Next Word vFinal.R")
#prediction1 <- predict_nextword(c("how"),100,0,5) 
```

```{r predict_uni2, echo=TRUE, results='asis'} 
# Print the basic information about the files.  
#kable(prediction1)
```

### 6.2 Example with Bigram

```{r predict_bi1,echo=TRUE} 
#prediction2 <- predict_nextword(c("how","are"),100,0,5) 
```

```{r predict_bi2,echo=TRUE, results='asis'} 
# Print the basic information about the files.  
#kable(prediction2)
```

### 6.3 Example with Trigram

```{r predict_tri1,echo=TRUE} 
#prediction3 <- predict_nextword(c("how","are","you"),100,0,5) 
```

```{r predict_tri2,echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(prediction3)
```


## 7. Prediction of next word using Regex

```{r predict_regex,echo=TRUE} 
#source("Pred Next Word vFinal.R")
#source("Pred Next Word Regex vFinal.R")
#prediction1 <- predict_nextword_regex(c("how","are",""),100,0,5)
#prediction2 <- predict_nextword(c("how","are"),100,0,5)
```


```{r echo=TRUE, results='asis'}
# Print the basic information about the files. 
#kable(prediction1)
#kable(prediction2)
```
