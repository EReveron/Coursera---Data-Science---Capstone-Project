{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Capstone Project - Milestone Exploratory Data Analysis Report\"\nauthor: \"Enrique Reveron\"\ndate: \"June 12, 2016\"\noutput:\n  html_document:\n    fig_height: 4\n    fig_width: 9\n    keep_md: yes\n    theme: default\n  pdf_document: default\n  word_document: default\n---\n\n\n```{r global_options, include=FALSE}\nknitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=FALSE)\n```\n\n## Executive Summary\n\nThis is a Milestone report related with the Coursera Capstone Project, the target is show initial exploratory data analysis about the US dataset that include three kind of files:\n\n* Twitter\n* News\n* Blogs\n\n\n## 1. Load the Neccesary Libraries\n\nFor this project we will use basicly the **quanteda**,**ggplot2**, **knitr** and **RColorBrewer**.\n\n```{r echo=TRUE}\nlibrary(quanteda)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(RColorBrewer)\n\nsetwd(\"D:/001 -- Coursera/Capstone Project/Coursera-SwiftKey/final/en_US\")\nset.seed(12345) # For reproducibility\n\n```\n\n## 2. Exploratory Data Analysis\n### 2.1 Basic Files Information\n\nThe files that we will use in the project are bigger than **150Mbytes** each one. In order to do the exploratory data analysis and to have an acceptable runtime, I will use only **10%** of the data. \n\n```{r echo=FALSE, cache = TRUE}\n# Let's create a data table to show the results of the Exploratory Analisys\ndt <- as.data.frame(c(filename_twitter <- \"en_US.twitter.txt\",\n                      filename_news <- \"en_US.news.txt\", \n                      filename_blogs <- \"en_US.blogs.txt\"))\ncolnames(dt)[1] <- \"Filename\"\n\n# Size of each file\ndt[,2] <- c(size.twitter.data <- file.size(filename_twitter),\n            size.news.data <- file.size(filename_news),\n            size.blogs.data <- file.size(filename_blogs))\ncolnames(dt)[2] <- \"Filesize\"\n\n# Load the Data\ntwitter.data <- readLines(filename_twitter, encoding=\"UTF-8\", warn = FALSE)\nnews.data <- readLines(filename_news, encoding=\"UTF-8\", warn = FALSE)\nblogs.data <- readLines(filename_blogs, encoding=\"UTF-8\", warn = FALSE)\n\n# Number of lines\ndt[,3] <- c(lines.twitter.data <- length(twitter.data),\n            lines.news.data <- length(news.data),\n            lines.blogs.data <- length(blogs.data))\ncolnames(dt)[3] <- \"Total Lines\"\n\n## In order to do this report, we will use only 10% of the data\ntwitter.data <- sample(twitter.data, size = lines.twitter.data * 0.01, replace = FALSE)\nnews.data <- sample(news.data, size = lines.news.data * 0.01, replace = FALSE)\nblogs.data <- sample(blogs.data, size = lines.blogs.data * 0.01, replace = FALSE)\n\ndt[,4] <- c(lines.twitter.data <- length(twitter.data),\n            lines.news.data <- length(news.data),\n            lines.blogs.data <- length(blogs.data))\ncolnames(dt)[4] <- \"Subset Lines\"\n```\n\nThose are the main characteristics of the files:\n\n```{r echo=TRUE, results='asis'}\n# Print the basic information about the files. \nkable(dt)\n```\n\nAccording with this, we will use only *23601*, *772* and *8992* lines of **twitter**, **news** and **blogs** datasets. \n\n### 2.2 File Content   \n\nLet's see some some examples of the content for each of the files:\n\n```{r echo=TRUE}\ntwitter.data[1:3]\nnews.data[1:3]\nblogs.data[1:3]\n```\n\n\n### 2.3 Character Basis Analysis \n\n```{r echo=FALSE}\nlines_char.twitter <- as.data.frame(nchar(twitter.data[1:lines.twitter.data]))\nlines_char.twitter[,2] <- \"twitter\"\ncolnames(lines_char.twitter) <- c(\"num_char\",\"type\")\n\nlines_char.blogs <- as.data.frame(nchar(blogs.data[1:lines.blogs.data]))\nlines_char.blogs[,2] <- \"blogs\"\ncolnames(lines_char.blogs) <- c(\"num_char\",\"type\")\n\nlines_char.news <- as.data.frame(nchar(news.data[1:lines.news.data]))\nlines_char.news[,2] <- \"news\"\ncolnames(lines_char.news) <- c(\"num_char\",\"type\")\n\nlines_char.all <- rbind(lines_char.news, \n                        lines_char.blogs, lines_char.twitter)\n\n# Maximun, Average and Minimun number of Characters \ndt[,5] <- c(max_char.twitter <- max(lines_char.twitter$num_char),\n            max_char.news <- max(lines_char.news$num_char),\n            max_char.blogs <- max(lines_char.blogs$num_char))\ncolnames(dt)[5] <- \"Max Chars per Line\"\n\ndt[,6] <- c(avg_char.twitter <- median(lines_char.twitter$num_char),\n            avg_char.news <- median(lines_char.news$num_char),\n            avg_char.blogs <- median(lines_char.blogs$num_char))\ncolnames(dt)[6] <- \"Avg Chars per Line\"\n\ndt[,7] <- c(min_char.twitter <- min(lines_char.twitter$num_char),\n            min_char.news <- min(lines_char.news$num_char),\n            min_char.blogs <- min(lines_char.blogs$num_char))\ncolnames(dt)[7] <- \"Min Chars per Line\"\n```\n\nLet's see some information regarding the amount of characters per line for each dataset:\n\n```{r echo=TRUE, results='asis'}\nkable(dt)\n```\n\nWe can see that the longest line belongs to **blogs** dataset (*4596*), but the longest in average is the **news** dataset (*188*), and the maximun for **twitter** is already *140* (the maximun amount defined in the social media platform, until now). \n\nLet's make some plots in order to see that:\n\n```{r echo=TRUE}\n# Print Histogram of Number of Characters per Line\nggplot(data=lines_char.all, aes(x=num_char, fill=type)) +\n  geom_histogram() +\n  facet_wrap(~ type, ncol = 1, scales=\"free\") +\n  labs(title=\"Histogram for Number of Characters per Line\") +\n  labs(x=\"Number of Characters\",y=\"Number of Lines\") \n\nggplot(data=lines_char.all, aes(x=num_char, fill=type, colour=type)) +\n  geom_freqpoly() +\n  labs(title=\"Histogram for Number of Characters per Line\") +\n  labs(x=\"Number of Characters\",y=\"Number of Lines\") \n```\n\n\n\n### 2.4 Basic Words Analysis \n\nLet's see some characteristics about the amount of words per line. For this step, we consider a \"word\" any group of characters separated by \" \". We will use the following function to count the number of words per line:\n\n\n```{r echo=TRUE}\nf_num_words <- function(x) length(unlist(strsplit(x,split=\" \")))\n```\n\nLet's see the minimun, average and maximun amount of words per line for each type of files:\n\n```{r echo=FALSE}\nlines_word.twitter <- as.data.frame(unlist(lapply(twitter.data, f_num_words)))\nlines_word.twitter[,2] <- \"twitter\"\ncolnames(lines_word.twitter) <- c(\"num_words\",\"type\")\n\nlines_word.blogs <- as.data.frame(unlist(lapply(blogs.data, f_num_words)))\nlines_word.blogs[,2] <- \"blogs\"\ncolnames(lines_word.blogs) <- c(\"num_words\",\"type\")\n\nlines_word.news <- as.data.frame(unlist(lapply(news.data, f_num_words)))\nlines_word.news[,2] <- \"news\"\ncolnames(lines_word.news) <- c(\"num_words\",\"type\")\n\nlines_word.all <- rbind(lines_word.news,lines_word.blogs,lines_word.twitter)\n\n# Maximun, Average and Minimun number of Words \ndt[,8] <- c(max(lines_word.twitter$num_words),\n            max(lines_word.news$num_words),\n            max(lines_word.blogs$num_words))\ncolnames(dt)[8] <- \"Max Words per Line\"\n\ndt[,9] <- c(median(lines_word.twitter$num_words),\n            median(lines_word.news$num_words),\n            median(lines_word.blogs$num_words))\ncolnames(dt)[9] <- \"Avg Words per Line\"\n\ndt[,10] <- c(min(lines_word.twitter$num_words),\n            min(lines_word.news$num_words),\n            min(lines_word.blogs$num_words))\ncolnames(dt)[10] <- \"Min Words per Line\"\n```\n\n\n```{r echo=TRUE, results='asis'}\nkable(dt)\n```\n\nThe results are similar to the previous character analysis, in terms of *average words per line*, **blogs** and **news** are very similar (*29* and *32*), and the *maximun words per line* for **blogs** dataset is very big compare with the others (*715* compare to *135* and *39*).\n\nLet's see that information with some plots:\n\n```{r echo=TRUE}\nggplot(data=lines_word.all,aes(x=num_words, fill=type)) +\n  geom_histogram() +\n  facet_wrap(~ type, ncol = 1, scales=\"free\") +\n  labs(title=\"Histogram for Number of Words per Line\") +\n  labs(x=\"Number of Words per Line\",y=\"Frequency\") \n\nggplot(data=lines_word.all,aes(x=num_words,fill=type,colour=type)) +\n  geom_freqpoly() +\n  labs(title=\"Histogram for Number of Words per Line\") +\n  labs(x=\"Number of Words per Line\",y=\"Frequency\") \n```\n\n### 2.5 Corpora\n\nLet's built the corpora for each of the files to be used in future analysis, using the *corpus()* function of **quanteda** library:\n\n```{r echo=TRUE}\ntwitter.docvars <- data.frame(Source = rep(\"twitter\",lines.twitter.data))\nblogs.docvars <- data.frame(Source = rep(\"blogs\",lines.blogs.data))\nnews.docvars <- data.frame(Source = rep(\"news\",lines.news.data))\n\ntwitter.corpus <- corpus(twitter.data, docvars = twitter.docvars)\nnews.corpus <- corpus(news.data, docvars = news.docvars)\nblogs.corpus <- corpus(blogs.data, docvars = blogs.docvars)\n\n## Let's see information about the corpus\nsummary(twitter.corpus,1)\nsummary(news.corpus,1)\nsummary(blogs.corpus,1)\n```\n\nWe can see that the **twitter**, **news** and **blogs** *Corpus* have *23601**, **772* and *8992* documents (equal to number of lines of each dataset).\n\nWith **quanteda** packages is very simple to create a new Corpus combining the previous ones:\n\n```{r echo=TRUE}\nall.corpus <- (twitter.corpus + news.corpus) + blogs.corpus\nsummary(all.corpus,1)\n```\n\nWe can see that this new Corpus have have *33365* documents (equal to add the number of documents of each corpus).\n\n### 2.6 Word Analysis with Document Feature Matrix\n\nLet's built the document-feature matrix using the *dfm()* function to analyze the features and frequencies. We will also clean the data by doing the following: \n\n* Lower all the characters (*toLower = TRUE*)\n* Remove numbers (*removeNumbers = TRUE*)\n* Remove punctuation symbols (*removePunct = TRUE*)\n* Remove separators (*removeSeparators = TRUE*)\n* Remove twitter characters (*removeTwitter = TRUE*)\n* Remove bad/profanity english words (*stopwords(\"english\")*)\n\n\n```{r echo=TRUE}\nall.dfm <- dfm(all.corpus, \n             toLower = TRUE,\n             removeNumbers = TRUE, \n             removePunct = TRUE, \n             removeSeparators = TRUE,\n             removeTwitter = TRUE, \n             stem = FALSE, \n             language = \"english\",\n             ignoredFeatures = stopwords(\"english\"))\n## Total number of features (words)\nprint(num.words.all <- nfeature(all.dfm))\n## Information of dfm\nhead(all.dfm,5)\n```\n\nWe can see that the total number of features is *44119*, that's is the number of words that are included in the Corpora.\n\n### 2.6.1 Top 20 Words\n\nWe can use the data feature matrix created previously to analyze the *frequency* and *accumulated frequency* of words in the Copora.\n\n```{r echo=FALSE}\nall.words <- data.frame(topfeatures(all.dfm,num.words.all))\ncolnames(all.words)[1] <- \"freq\"\n\nall.words[,2] <- cumsum(all.words)\ncolnames(all.words)[2] <- \"acumfreq\"\n\ntotal.words <- sum(all.words$freq)\n\nall.words[,3] <- all.words$freq / total.words * 100\ncolnames(all.words)[3] <- \"perfreq\"\n\nall.words[,4] <- all.words$acumfreq / total.words * 100\ncolnames(all.words)[4] <- \"acumperfreq\"\n\nall.words[,5] <- rownames(all.words)\ncolnames(all.words)[5] <- \"word\"\n\nall.words[,6] <- seq.int(nrow(all.words))\ncolnames(all.words)[6] <- \"numword\"\n```\n\nLet's see the top-20 words and the frequency related information:\n\n```{r echo=TRUE}\nhead(all.words,20)\n```\n\nWe can see the top-20 words, the meaning of each of the columns is:\n\n* *freq*: number of times that the word appears in the Corpora.\n\n* *acumfreq*: accumulated frequency (considering the previous n-1 words).\n\n* *perfreq*: percentage frequency of the work in the Corpora. \n\n* *acumperfreq*: accumulated percentage frequency (considering the previous n-1 words).\n\n* *numwords*: the number of words included in the accumulated values.\n\nFor example, we can see that the word **now** in row **11**, appears *1469* times into the Corpora (*freq*), this value is equivalent to *0,39%* of the total corpora size (*perfreq*). Because the table is sorted, the *acumulated frequency* of the word **now** is *20839*, it means the sum of all the previous words *frecuencies* in the table plus it's *freq* value: *19370 + 1469 = 20839* (*acumfreq*) that is equivalent to *5.55%* of the total corpora size (*acumperfreq*). Also, we can see that in order to cover the *5.55%* of the Corpora we need only **11** words (*numword*).\n\nLet's plot the top-20 words and also make the worcloud:\n\n```{r echo=TRUE}\n## Plot Top 20 Words\nggplot(data=head(all.words,20),\n       aes(x=reorder(word,-freq), y=freq)) +\n  geom_bar(stat =\"identity\", position= \"identity\") +\n    theme(axis.text.x = element_text(angle = 90, vjust = 0, hjust = 1,size=12)) +\n  labs(title=\"Top 20 Words\") +\n  labs(x=\"Top Words\",y=\"Count\") \n\n## Plot Worcloud with top 20 words\nplot(all.dfm, max.words = 20, random.order = FALSE, colors = brewer.pal(6, \"Dark2\"))\n```\n\n### 2.7 Amount of Unique Words needed to Cover all Word Instances in the Language\n\nWe can use the previous information to validate how many unique words we need in order\nto cover **50%** and **90%** of the total language (the *numwords* associated with the desired *acumfreq* value): \n\n```{r echo=TRUE}\nhead(subset(all.words, acumperfreq >= 50 & acumperfreq <= 51),1)\nhead(subset(all.words, acumperfreq >= 90 & acumperfreq <= 91),1)\n```\n\nWe can see that we only need aproximately **820** words in order to cover the **50%**, and **13493** to cover the **90%**.\n\nIn the following plot we can observe this:\n\n```{r echo=TRUE}\nggplot(data=all.words, aes(x=numword, y=acumperfreq)) +\n  geom_line(stat =\"identity\", position= \"identity\",size=1.2, colour=\"black\") +\n  geom_text(data=subset(all.words, numword == 820 | numword == 13493),\n            aes(label=paste(\"(\",acumperfreq,\",\",numword,\")\")), \n            hjust = 1.2, vjust = -0.4) +\n  geom_vline(xintercept = 820, color=\"red\") +\n  geom_hline(aes(yintercept=50), color=\"red\") +\n  geom_vline(xintercept = 13493, color=\"blue\") +\n  geom_hline(aes(yintercept=90), color=\"blue\") +\n  labs(title=\"Number of Words needed to Cover all Words Instances\") +\n  scale_x_continuous( trans = \"log10\") +   \n  labs(x=\"Number of Words\",y=\"%Coverage\")\n```\n\nWe can use those results in the prediction algorithm in order to speed up the processing time. \n\n## 2.8 N-grams\n\nLet's analyze the n-grams of the dataset. \n\n### 2.8.1 Uni-grams\n\nIn order to check the unigrams, bigrams and trigrams of the all dataset, we will use the function *dfm()* from **quanteda** using the *ngram* option parameter. \n\nBy default, the *dfm()* function calculate the unigrams of the texts, so the results that we got in the section **2.6 Word Analysis with Document Feature Matrix** correspond to the **unigrams** of the Corpora.  \nLet's calculate the Bigrams and Trigrams of the corpora and plot the top-20 n-grams and wordcloud for each one:\n\n### 2.8.2 Bigrams\n\nWe will calculate the **bigrams** of the Corpora trought the *dfm()* function, using the same cleaning options for the unigrams.\n\n```{r echo=TRUE}\nall.bigrams.dfm <- dfm(all.corpus, \n             toLower = TRUE,\n             removeNumbers = TRUE, \n             removePunct = TRUE, \n             removeSeparators = TRUE,\n             removeTwitter = TRUE, \n             stem = FALSE, \n             language = \"english\",\n             ignoredFeatures = stopwords(\"english\"),\n             ngrams=2)\n```\n\nWe can see that exists **313479** features.\n\nLet's plot the **top-20 bigrams** and the *wordcloud* related:\n\n```{r echo=TRUE}\ntop20.bigrams <- data.frame(topfeatures(all.bigrams.dfm,20))\ncolnames(top20.bigrams)[1] <- \"freq\"\n\ntop20.bigrams[,2] <- rownames(top20.bigrams)\ncolnames(top20.bigrams)[2] <- \"bigrams\"\n\nggplot(data=top20.bigrams,aes(x=reorder(bigrams,-freq), y=freq)) +\n  geom_bar(stat =\"identity\", position= \"identity\") +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title=\"Top 20 Bigrams\") +\n  labs(x=\"bigrams\",y=\"Count\") \n\n## Plot Worcloud with top 20 words\nplot(all.bigrams.dfm, max.words = 20, random.order = FALSE, \n     rot.per=0.35,scale=c(3,0.5),\n     colors = brewer.pal(6, \"Dark2\"))\n```\n\n### 2.8.3 Trigrams\n\nLet's calculate the **trigrams** of the Corpora, using the same *dfm()* function:\n\n```{r echo=TRUE}\nall.trigrams.dfm <- dfm(all.corpus, \n             toLower = TRUE,\n             removeNumbers = TRUE, \n             removePunct = TRUE, \n             removeSeparators = TRUE,\n             removeTwitter = TRUE, \n             stem = FALSE, \n             language = \"english\",\n             ignoredFeatures = stopwords(\"english\"),\n             ngrams=3)\n```\n\nWe can see that exists **531460** features.\n\nLet's plot the **top-20 trigrams** and the *wordcloud* related:\n\n```{r echo=TRUE}\ntop20.trigrams <- data.frame(topfeatures(all.trigrams.dfm,20))\ncolnames(top20.trigrams)[1] <- \"freq\"\n\ntop20.trigrams[,2] <- rownames(top20.trigrams)\ncolnames(top20.trigrams)[2] <- \"trigrams\"\n\nggplot(data=top20.trigrams,aes(x=reorder(trigrams,-freq), y=freq)) +\n  geom_bar(stat =\"identity\", position= \"identity\") +\n    theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  labs(title=\"Top 20 Trigrams\") +\n  labs(x=\"trigrams\",y=\"Count\")\n## Plot Worcloud with top 20 words\nplot(all.trigrams.dfm, max.words = 20, random.order = FALSE, scale=c(2,0.5),\n     colors = brewer.pal(6, \"Dark2\"))\n```\n\nWe can see that some of the trigrams should be cleaned, words like **happy mother's day** and **happy mothers day** are the same and should be considered as equal, also words like **please please please** and **love love love** must be handled. This will be take into consideration for next steps.\n\n## 3. How to Identify and Clean Non-english Words (Work in progress)\n\nOne option to filter the Corpora and remove the **non-english words** will be using a english dictionary to do that. Is neccesary to find a english dictionary and work on it.\n\nAnother choice is a library named **textcat** that make text cathegorization based on n-grams thought the function *textcat()*.\nLet's see how this works:\n\n```{r echo=TRUE}\nlibrary(textcat)\ntextcat(c(\"This is a english sentence\",\n          \"Esto es una oracion en espanol\",\n          \"This is esto es datos\",\n          \"madre\",\n          \"father\",\n          \"bonjour\",\n          \"merci\"),\n        p=ECIMCI_profiles)\n```\n\nIs working good but is not perfect. Let's see how this works with our data:\n\n```{r echo=TRUE}\nall.tokens <- toLower(\n  tokenize(all.corpus, what = \"fasterword\",\n         removeNumbers = TRUE, \n         removePunct = TRUE, \n         removeSeparators = TRUE,\n         removeTwitter = TRUE, \n         removeURL = TRUE))\nall.tokens[1:8]     \ntextcat(all.tokens[1:8], p=ECIMCI_profiles)\n```\n\nThere still some problems that I need to fix regarding characters encoding (some characters that the function doesn't handle). This texts is a good example of this issue, the function doesn't work with it:\n```{r echo=TRUE}\nall.corpus[9]\n```\n\n\n## 4. Future Work - Prediction Algorithm\n\nThe next steps that I will perform to create a prediction algortihm will be:\n\n* Considering how to fix the issues with the *textcat()* function to remove the non-english words.\n\n* Considering how to reduce the amount of n-grams using steamming or addiotional filtering to improve the runtime of the prediction algorithm.\n\n* Consider how to create the prediction algorithm using the n-grams and how to predict the words that can not be handle by the algorithm perse.\n\n* Design the GUI of the Shiny App and star working on it, taking into consideration runtime and memory restrictions.\n",
    "created" : 1467120356413.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3314233058",
    "id" : "7DDAE4EC",
    "lastKnownWriteTime" : 1467118504,
    "last_content_update" : 1467118504,
    "path" : "D:/Coursera/Capstone Project/Coursera---Data-Science---Capstone-Project/Middle Report.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}