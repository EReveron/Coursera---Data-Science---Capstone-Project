{
    "collab_server" : "",
    "contents" : "#########################################################################\n# Performance Test\n#     We will test the performance in terms of memory ussage and computing\n#  time in order to be implemented in the final project.\n# We will test quanteda package in terms of:\n#    \n#  dfm:\n#       * Use Corpus vs TokenizedText to create a dfm\n#       Remove bad words on dfm by: \n#            Remove previusly on TokenizedTexts (selectedFeatures) vs\n#            Remove directly on dfm creation (ignoredFeatures)\n#\n#  data.table vs data.frame manipulations computing time\n\n\nlibrary(quanteda)\nlibrary(data.table)\nlibrary(profr)\nlibrary(utils)\nlibrary(profvis)\n\nsetwd(\"D:/Coursera/Capstone Project/Coursera-SwiftKey/final/en_US/FINAL2\")\nprofanityList <- c(\"shit\",\"piss\",\"fuck\",\"cunt\",\"cocksucker\",\"motherfucker\",\"tits\")\n\n\nmemory.size()\nmemory.size(TRUE)\nmemory1 <- gc()\n\nfilename_twitter <- \"en_US.twitter.txt\"\ntext.data <- readLines(filename_twitter, n = 50000, encoding=\"UTF-8\", warn = FALSE)\n# remove emojies and other characters.\ntext.data <- iconv(text.data, \"latin1\", \"ASCII\", sub=\"\")\ntext.data <- iconv(text.data, \"ISO-8859-2\", \"ASCII\", sub=\"\")\n\nobject.size(text.data)\nsave(\"text.data\",file=\"text_data.Rds\")\n\n\n##################### 1. Using Corpus vs TokenizedText ####################\n##################### 1.1.1 Corpus \n\n\ntest_corpus <- function(n) {\n  mycorpus <- corpus(text.data)\n  uni.corpus.dfm <- dfm(mycorpus, \n                        what = \"fastestword\", \n                        verbose = FALSE,\n                        toLower = TRUE,\n                        removeNumbers = TRUE, \n                        removePunct = TRUE,\n                        removeSymbols = TRUE, \n                        removeSeparators = TRUE, \n                        removeTwitter = TRUE,\n                        removeHyphens = TRUE, \n                        removeURL = TRUE,\n                        stem = FALSE,\n                        ngrams = n, \n                        concatenator = \"_\",\n                        ignoredFeatures = profanityList,\n                        valuetype = \"regex\")\n}\n\n\nload(\"text_data.Rds\")\n\nprint(\"Corpus Unigram:\")\nsystem.time(test_corpus(1))\nprint(\"Corpus Bigram:\")\nsystem.time(test_corpus(2))\n#system.time(test_corpus(3))\n#system.time(test_corpus(4))\n\nprint(\"Corpus Unigram:\")\np.corpus_uni <- profvis({ test_corpus(1) })\nprint(\"Corpus Bigram:\")\np.corpus_bi <- profvis({ test_corpus(2) })\nprint(\"Corpus Trigram:\")\np.corpus_tri <- profvis({ test_corpus(3) })\nprint(\"Corpus Quadgram:\")\np.corpus_quad <- profvis({ test_corpus(4) })\n\n#htmlwidgets::saveWidget(p.corpus, \"ProfvisCorpusUni.html\")\n#save(\"uni.corpus.dfm\",file=\"uni_corpus_dfm.Rdata\")\n#rm(\"mycorpus\",\"uni.corpus.dfm\")\n#rm(\"p.corpus\")\n#gc()\n\n\n\n\n\n##################### 1.2.1 TokenizedText with Unigrams\n\ntest_tokenized <- function(n) {  \n  text.data <- toLower(text.data)\n  uni.token <- tokenize(text.data,\n                      what = \"fastestword\", \n                      verbose = FALSE,\n                      removeNumbers = TRUE, \n                      removePunct = TRUE,\n                      removeSymbols = TRUE, \n                      removeSeparators = TRUE, \n                      removeTwitter = TRUE,\n                      removeHyphens = TRUE, \n                      removeURL = TRUE,\n                      ngrams = n, \n                      concatenator = \"_\")\n\n  uni.token.dfm <- dfm(uni.token, \n                      what = \"fastestword\", \n                      verbose = FALSE,\n                      toLower = FALSE,\n                      removeNumbers = FALSE, \n                      removePunct = FALSE,\n                      removeSymbols = FALSE, \n                      removeSeparators = FALSE, \n                      removeTwitter = FALSE,\n                      removeHyphens = FALSE, \n                      removeURL = FALSE,\n                      stem = FALSE,\n                      ngrams = n, \n                      concatenator = \"_\",\n                      ignoredFeatures = profanityList,\n                      valuetype = \"regex\")\n}\n  \n\nload(\"text_data.Rds\")\n\nprint(\"Tokenized Unigram:\")\nsystem.time(test_tokenized(1))\nprint(\"Tokenized Bigram:\")\nsystem.time(test_tokenized(2))\n#system.time(test_tokenized(3))\n#system.time(test_tokenized(4))\n\nprint(\"Tokenized Unigram:\")\np.corpus_uni <- profvis({ test_tokenized(1) })\nprint(\"Tokenized Bigram:\")\np.corpus_bi <- profvis({ test_tokenized(2) })\nprint(\"Tokenized Trigram:\")\np.corpus_tri <- profvis({ test_tokenized(3) })\nprint(\"Tokenized Quadgram:\")\np.corpus_quad <- profvis({ test_tokenized(4) })\n\n\n##################### 1.3.1 TokenizedText + RemoveFeatures with Unigrams\n\ntest_tokenized_remove <- function(n) {\n  text.data <- toLower(text.data)\n  uni.token <- tokenize(text.data,\n                          what = \"fastestword\", \n                          verbose = FALSE,\n                          removeNumbers = TRUE, \n                          removePunct = TRUE,\n                          removeSymbols = TRUE, \n                          removeSeparators = TRUE, \n                          removeTwitter = TRUE,\n                          removeHyphens = TRUE, \n                          removeURL = TRUE,\n                          ngrams = 1L, \n                          concatenator = \"_\")\n  uni.token <- removeFeatures(uni.token, profanityList)\n  uni.token.dfm <- dfm(uni.token, \n                         what = \"fastestword\", \n                         verbose = FALSE,\n                         toLower = FALSE,\n                         removeNumbers = FALSE, \n                         removePunct = FALSE,\n                         removeSymbols = FALSE, \n                         removeSeparators = FALSE, \n                         removeTwitter = FALSE,\n                         removeHyphens = FALSE, \n                         removeURL = FALSE,\n                         stem = FALSE,\n                         ngrams = n, \n                         concatenator = \"_\")\n}\n\nload(\"text_data.Rds\")\n\nprint(\"Tokenized + Remove Unigram:\")\nsystem.time(test_tokenized_remove(1))\nprint(\"Tokenized + Remove Bigram:\")\nsystem.time(test_tokenized_remove(2))\n#system.time(test_tokenized_remove(3))\n#system.time(test_tokenized_remove(4))\n\nprint(\"Tokenized + Remove Unigram:\")\np.corpus_uni <- profvis({ test_tokenized_remove(1) })\nprint(\"Tokenized + Remove Bigram:\")\np.corpus_bi <- profvis({ test_tokenized_remove(2) })\nprint(\"Tokenized + Remove Trigram:\")\np.corpus_tri <- profvis({ test_tokenized_remove(3) })\nprint(\"Tokenized + Remove Quadgram:\")\np.corpus_quad <- profvis({ test_tokenized_remove(4) })\n\n\n#htmlwidgets::saveWidget(p.token.selected, \"ProfvisTokenSelecUni.html\")\n#save(\"uni.token.dfm\",file=\"uni_token_dfm_selec.Rdata\")\n#rm(\"uni.token\",\"uni.token.dfm\")\n#rm(\"p.token.selected\")\n#gc()\n\n\n\n###################### Opcion 3#####################################\n\n\n\n\n\n\ntest_tokenized_remove_ngram <- function(n) {  \n  text.data <- toLower(text.data)\n  uni.token <- tokenize(text.data,\n                        what = \"fastestword\", \n                        verbose = FALSE,\n                        removeNumbers = TRUE, \n                        removePunct = TRUE,\n                        removeSymbols = TRUE, \n                        removeSeparators = TRUE, \n                        removeTwitter = TRUE,\n                        removeHyphens = TRUE, \n                        removeURL = TRUE,\n                        ngrams = 1L)\n  \n  uni.token <- removeFeatures(uni.token, profanityList)\n  \n  uni.token <- ngrams(uni.token, n)\n  \n  uni.token.dfm <- dfm(uni.token, \n                       what = \"fastestword\", \n                       verbose = FALSE,\n                       toLower = FALSE,\n                       removeNumbers = FALSE, \n                       removePunct = FALSE,\n                       removeSymbols = FALSE, \n                       removeSeparators = FALSE, \n                       removeTwitter = FALSE,\n                       removeHyphens = FALSE, \n                       removeURL = FALSE,\n                       stem = FALSE,\n                       ngrams = n, \n                       concatenator = \"_\")\n}\n  \n  \n  \n\n#htmlwidgets::saveWidget(p.token.selected, \"ProfvisTokenRemoveUniNgram.html\")\n#save(\"uni.token.dfm\",file=\"uni_token_dfm_remove_ngram.Rdata\")\n#rm(\"uni.token\",\"uni.token.dfm\")\n#rm(\"p.token.selected\")\n#gc()\n\nload(\"text_data.Rds\")\nprint(\"Tokenized + Remove + Ngram Unigram:\")\nsystem.time(test_tokenized_remove_ngram(1))\nprint(\"Tokenized + Remove + Ngram Bigram:\")\nsystem.time(test_tokenized_remove_ngram(2))\n#system.time(test_tokenized_remove_ngram(3))\n#system.time(test_tokenized_remove_ngram(4))\n\n\n\n##################################\n  \nload(\"text_data.Rds\")\nprint(\"Corpus Unigram:\")\nsystem.time(test_corpus(1))\n\nload(\"text_data.Rds\")\nprint(\"Tokenized Unigram:\")\nsystem.time(test_tokenized(1))\n\nload(\"text_data.Rds\")\nprint(\"Tokenized + Remove Unigram:\")\nsystem.time(test_tokenized_remove(1))\n\nload(\"text_data.Rds\")\nprint(\"Tokenized + Remove + Ngram Unigram:\")\nsystem.time(test_tokenized_remove_ngram(1))\n\n########################\n\nload(\"text_data.Rds\")\nprint(\"Corpus Bigram:\")\nsystem.time(test_corpus(2))\n\n\n\nload(\"text_data.Rds\")\nprint(\"Tokenized Bigram:\")\nsystem.time(test_tokenized(2))\n\n\n\nload(\"text_data.Rds\")\nprint(\"Tokenized + Remove Bigram:\")\nsystem.time(test_tokenized_remove(2))\n\n\n\nload(\"text_data.Rds\")\nprint(\"Tokenized + Remove + Ngram Bigram:\")\nsystem.time(test_tokenized_remove_ngram(2))\n",
    "created" : 1467399374613.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3578914118",
    "id" : "42982FA0",
    "lastKnownWriteTime" : 1466629976,
    "last_content_update" : 1466629976,
    "path" : "D:/Coursera/Capstone Project/Coursera-SwiftKey/final/en_US/FINAL2/Performance Testv2.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 11,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}