{
    "contents" : "---\ntitle: \"Capstone Project - Final Report\"\nauthor: \"Enrique Reveron\"\ndate: \"July 16, 2016\"\noutput:\n  html_document:\n    fig_height: 4\n    fig_width: 9\n    fig_caption: yes\n    keep_md: yes\n    theme: default\n  pdf_document: default\n  word_document: default\n---\n\n\n```{r global_options, include=FALSE}\nknitr::opts_chunk$set(echo=FALSE, warning=FALSE, message=TRUE)\n```\n\n## Executive Summary\n\nThis is a Final Report related with the Switfkey Coursera Capstone Project, the target was create a Shiny App to predict next word using a US dataset that include three kind of files:\n\n* Twitter\n* News\n* Blogs\n\nThis Final Report will explain the most relevant considerations to finally build the application. As a Natural Language Processing (NLP) Application, the core is define how to calculate the probability to each word that could be used in the prediction task.\n\nAs was show in the **Milestone Report**, the dataset (Twitter, News and Blogs US files) is considerable big to be handle by a normal PC Computer, also as a final target, the prediction application must run on Shiny servers on the Internet, so is very important to consider how to simplify and optimize the process to provide a solution that suits the enviroment. The main assumptions, simplifications and optimizations are described in the next section. \n\n## 1. Cleaning and Transform the Dataset (Corpora)\n\nOne special and important step is how to clean and transform the Dataset. In NLP, the dataset is named the **corpus** (or **corpora** in plural).\nThis project only consider to use the English language corpora, as a part of the cleaning stage, we consider the following:\n\n### 1.1. Subsetting the Corpora\n\nFollowing the idea of Machine Learning Algorithms and the already mentioned limited computational resources, we consider to take a subset of the corpora, **80% of the total** to be used as a **training data**.\n\n### 1.2. Transform ISO8859-2 and Latin Characters\n\nSome special characters (**like emojies and latin characters**) was found in the corpora, so was neccesary to convert **all the text first to ASCII** to remove it.\n\n### 1.3. Lowering the Characters\n\nTo simplify the process, we consider to lowering all the corpora characters.\n\n### 1.4 Remove Special Words\n\nInto the corpora was found several special words that we remove it:\n\n* URL's: http://www.coursera.org, https://www.123.com\n* Email Addresses: enrique.reveron@gmail.com\n* Twitter Addresses and Hashtag: @aes, #SavetheWorld\n* Words that start with Digits: *123end, 1up, 2less*\n* Words that finish with Digits: *end123, up2, less4*\n* Digits: *12345, 12, 1345* and\n* Symbols: *+,-,=,[,],{,},?,*,*\n\n### 1.5 Strategy to Handle Special Characters\n\nAfter severals tests, we identify the neccesity to establish a strategy to handle two kind of special characters: **sentence separators ( , ; . ! ? : )** and **apostrophe ( ' )**. \n\nThe idea to handle the **sentence separators** raised because in the process to create the ngrams tables (called **tokenization**) appears fake ngrams that could result in a wrong prediction result. Let's make an example, if we consider a sentence into the corpora like this: \n\n   *\"how are you? i'm ok, i want to talk to you about $$$\"*\n   \nIf we consider as a cleaning strategy remove all the punctuation symbols (including the ones identified previously as a sentence separators), the sentence will be transform to:\n\n   *\"how are you i'm ok i want to talk to you about\"*\n\nThe **bigram tokenization** process will create the following **bigrams**:\n\n* (how are)\n* (are you)\n* (you i'm)\n* (i'm ok)\n* (ok i)\n* (i want)\n* ....\n\nWe can see that the bigrams *(you i'm)* and *(ok i)* are fake ones (are not real bigrams into the corpora) and could affect the prediction task. To avoid this, we **substitute first the sentence separators by a special character** (**\"eeee\"**, that is not a regular english word and should not appear into the corpora), and then **substitute all the remaining symbols**. Using the previous example, after substitute the **sentence separators** by *\"eeee\"* we will have:\n\n   *\"how are you eeee i'm ok eeee i want to talk to you about\"*\n\nAnd the **bigrams** will be:\n\n* (how are)\n* (are you)\n* (you eeee)\n* (eeee i'm)\n* (i'm ok)\n* (ok eeee)\n* (eeee i)\n* (i want)\n* ....\n\nAnd later in the **clean ngram process** (we will discuss about it later), we will remove any bigram that include **eeee** in order to remove those fake ngrams. This process also works for unigrams, trigrams and quadgrams.\n\nRegarding **apostrophe ( ' )**, the objective was the opposite, avoid to remove it from the corpora to have words like **\"i'm\" \"you're\" \"did'nt\"** into the ngrams tables. To do that we consider to do something similar as sentence separators. First we **substitute apostrophe by a special character**, **then remove all the symbols** and **finally put apostrophe back**. Considering our previous example, after substitute **sentence separators by eeee** we will **substitute apostrophe by ffff**, so we will have:\n\n   *\"how are you eeee i ffff m ok eeee i want to talk to you about $$$\"*\n\nThen we will **remove all the symbols**, the previous example will be transform to:\n\n   *\"how are you eeee i ffff m ok eeee i want to talk to you about\"*\n   \nAnd finally **put apostrofe back**:\n\n   *\"how are you eeee i'm ok eeee i want to talk to you about\"*\n\n\n\n## 2. Ngrams Model Selection\n\nThe basic of a Natural Language Processing (NLP) Application, is create a ngram model and calculate the probability for each word on it. A **ngram** is a sequence of words that appears into the dataset with n-length (unigram, bigram, trigram, quadgram and ngram for 1,2,3,4 and n-length). Using those ngrams and the probabilities calculated, we can \"predict\" the next word. Using the Markov Chains principle with a ngram model, **we can use the last n-1 words in order to predict the next n-word**.\n\nOne important step is **choose the ngram model that will be created**. Based on the application enviroment limitations (my local PC with only 16GB RAM and finally Shiny Servers on Internet), we consider to implement a **quadgram model**.\n\nBased on this we will create: **Unigrams, Bigrams, Trigrams and Quadgrams Tables; calculate the probability for each ngram and use it to predict next word**.\n\nWith this model, **we can use the last three words to predict the next word using Quadgram Probabilities Tables**.\n\n\n## 3. Build a Quadgram Model\n\nTo **build the quadgram model** we choose the **quanteda** library that provide good tools to create and handle ngrams and the corpora.\n\n###\t3.1. Inside **quanteda** Library\n\nThe **quanteda** library provide several good tools that help us to build our quadgram model. In fact, the library offer different ways (using different types of objects class) to do the same tasks. In order to optimize the running time and resources, I did some test with different object types to choose the best way to use it.    \n\n#### 3.1.1. Corpus vs Tokenizedtexts\n\nIn order to build the quadgram model, we finally want to know the frequency that each unigram, bigram, trigram and quadgram have in the corpora. To get that with **quanteda** we must create a **Document Feature Matrix (dfm)** (using **quanteda 'dfm' object**). \n\nWe have several options to create a **'dfm'** object in **quanteda**:\n\n* Create a **'corpus'** object first (using **corpus()** function) and then create a **'dfm'** object (using **dfm()** function)\n* Create a **'tokenizedtext'** object first (using **tokenize()** function), then create a **'ngrams'** (using **ngrams()** function) and finally a **'dfm'** object (using **dfm()** function) \n\nIn this step I got a lot of problems regarding the memory limits of my PC (16GB), so I decided to do several test and use the **profvis()** library to find a solution.\n\nAfter check the results, I saw that the first option **(corpus() --> dfm())** takes 30% more **running time** that the second option **(tokenize() --> ngrams() --> dfm()) (52780 ms vs 74770 ms)**, in terms of **peak memory allocation requirements** the first option needs almost the double **(456.5MB vs 220.7MB)** as you can see in the following picture:\n\n<div style=\"width:800px; height=600px\">\n![Image DFM with Corpus Flame](Coursera - Capstone - profvis dfm with corpus flame.jpg)\n![Image DFM with Token Flame](Coursera - Capstone - profvis dfm with tokenize flame.jpg)\n</div>\n\nWas even more interesting to see that in the first option **(corpus() --> dfm())** the **dfm()** function make a internal call to **tokenize()** function, that also make a internal call to **ngrams()** as you can see in the following picture:\n\n<div style=\"width:800px; height=600px\">\n![Image DFM with Corpus Data](Coursera - Capstone - profvis dfm with corpus data.jpg)\n![Image DFM with Token Data](Coursera - Capstone - profvis dfm with tokenize data.jpg)\n</div>\n\nBased on those results, we choose the second option **(tokenize() --> ngrams() --> dfm())** and was possible to create the dfm's for all ngrams (unigrams, bigrams, trigrams and quadgrams) in my computer.\n\n\n#### 3.1.2. Strategy to Remove Bad Words and Fake Ngrams  \n\nTo remove bad words and avoid to create fake ngrams, we have a similar approach to **sentence separators** **(please check 1.1.5 Strategy to Handle Special Characters, about \"Sentence Separators\")**, we will substitute into the corpora the bad words for the **sentence separator** special character **'eeee'** and finally remove all the ngrams that include it.\n\nTo remove those ngrams we also have several options in **quanteda**:\n\n* At **'ngram'** object creation time using **ngrams()** function with **ignoredFeatures** option \n* At **'dfm'** object creation time using **dfm()** function with **ignoredFeatures** option\n* After **'ngram'** object creation using **selectFeatures()** function with **selection = \"remove\"** and  **valuetype = \"regex\"** options\n\nBased on test results and memory restrictions, we choose the last option: **after ngrams() creation using selectFeatures() function with selection = \"remove\" and valuetype = \"regex\" options**.\n\n#### 3.1.3. Ngrams Creation\n\nIn order to create the Ngrams in **quanteda** we can also have several choices:\n\n* With **tokenize()** function using a 'character' object (the all corpora) with **ngrams** option\n* With **ngrams()** function using a 'character' object (the all corpora)\n* With **ngrams()** function using a 'tokenizedTexts' object \n\nBecause on **1.3.1.1 Corpus vs Tokenizedtexts** we already choose to create the **tokenizedTexts**, we use the third option. \n\nTo create each ngram (bigrams, trigrams, quadgrams) we **first create a 'tokenizedTexts' object** with ngram=1 and use this object to create the others. \n\n\n#### 3.1.4. Data Feature Matrix Trim\n\nBecause the big size of the different **'dfm' objects** (unigram, bigram, trigram and quadgram) we consider how to reduce the **'dfm' object size**, one option that **quanteda** provides is use the **trim()** function, is very flexible and you can choose to trim a dfm object based on document and term frequency, and/or subsampling.\n\nWe consider to **trim the dfm before** the Ngram probability calculation at the beggining, but because some wrong calculations appears (probability values larger than 1) we didn't use it at this level.\n\n### 4. Pre-compute Probability Tables\n\nOne obvius consideration is **pre-compute all ngrams probabilites** to avoid that the Shiny Application need to handle that, running faster on the enviroment. The next step is choose the probability calculation algorithm.\n\n### 5. Algorithm to be used to Calculate Ngram Probabilities\n\nThis is a very important step and maybe the most important one in the all process, we consider two options: **Maximun Likelihood (ML)** and **Kneser-ney Smoothing**.\n\n#### 5.1. Maximum Likelihood (ML)\n\nThis is the most simple one, it only consider the relationship between the frequency of each ngram with the total amount of ngrams.\n\n$$P_{ML}(w_{i}{\\mid}w_{i-n+1}^{i}) = \\frac{c(w_{i-n+1}^{i})}{\\sum_{w'_{i}} c(w_{i-n+1}^{i-1} w'_{i}) } = \\frac{c(w_{i-n+1}^{i})}{ c(w_{i-n+1}^{i-1})}$$\n\nWhere $c(w_{i-n+1}^{i})$ means the count (number of times) that the ngram $w_{i-n+1}^{i}$ appears on the corpora.\n\nThe calculations results are not so good to be used for prediction because the probability calculation only takes into consideration the same order ngram table, whithout taking into consideration the history (sequence).  \n\nFor example, suppose that we have the bigram **(in the)** with a frequence of **100**, and the frequency of the unigrams **(in)**  is *1000*, the **Maximun Likelihood (ML) Probability** is:\n\n$$P_{ML}(the {\\mid}in) = \\frac{c(in,the)}{\\sum_{w'_{i}} c(in, w'_{i}) } = \\frac{c(in,the)}{ c(in)} = \\frac{100}{1000} = 0,1$$\n\nAfter a research and some test, we consider to implement a better algorithm: Kneser-ney Smoothing Algorithm.\n\n#### 5.2. Kneser-ney Smoothing Algorithm\n\nThe Kneser-ney algorithm consider a smoothing interpolation mecanism and include into the calculation the low order ngrams table on that. \n\nAccording with the Kneser-ney algorithm, they define three different cases:\n\n#### 5.2.1. Lowest Order Equation\n\nFor lowest order **(n=1, unigrams)** the probability is:\n\n$${P_{KN}^{1}(w_{i})} = \\frac{N_{1+}({\\bullet} w_{i})} { N_{1+}({\\bullet} {\\bullet})}$$\n\nwhere:\n\n$$N_{1+}({\\bullet} w_{i}) = {\\mid}\\left\\{w_{i-1}:c(w_{i-1}^{i}) >0  \\right\\} {\\mid}$$\n$$N_{1+}({\\bullet} {\\bullet}) = {\\mid}\\left\\{(w_{i-1},w_{i}):c(w_{i-1}^{i}) >0  \\right\\} {\\mid} = \\sum_{w_{i}}N_{1+}({\\bullet} w_{i})$$\n\nIn other words, $N_{1+}({\\bullet} w_{i})$ is the number of words that **precede** $w_{i}$ at least once in the corpus.\n\n#### 5.2.2. Second Highest Ngrams to Bigrams Equation\n\nFrom second highest ngrams **(n-1)** to bigrams **(n=2)**,the following equation is used:\n\n$$P_{KN}^{n}(w_{i}{\\mid}w_{i-n+1}^{i}) = \n\\frac{max\\left\\{ N_{1+}(\\bullet w_{i-n+1}^{i} )-{\\delta}_{n},0\\right\\} }\n{ N_{1+}(\\bullet w_{i-n+1}^{i-1}\\bullet)} \n+ \\frac{{\\delta}_{n}} {N_{1+}(\\bullet w_{i-n+1}^{i-1}\\bullet)} \nN_{1+}(w_{i-n+1}^{i-1}\\bullet)P_{KN}^{n-1}(w_{i}{\\mid}w_{i-n+2}^{i-1})$$\n\nwhere:\n\n$$N_{1+}({\\bullet} w_{i-n+1}^{i}) = {\\mid}\\left\\{w_{i-n}:c(w_{i-n}^{i}) >0  \\right\\} {\\mid}$$\n\n$$N_{1+}(w_{i-n+1}^{i}{\\bullet} ) = {\\mid}\\left\\{w_{i-n+1}:c(w_{i-n+1}^{i+1}) >0  \\right\\} {\\mid}$$\n\n$$N_{1+}({\\bullet}w_{i-n+1}^{i-1} {\\bullet}) = \n{\\mid}\\left\\{(w_{i-n},w_{i}):c(w_{i-n}^{i}) >0  \\right\\} {\\mid} = \\sum_{{w'}_{i}}N_{1+}({\\bullet} {w}_{i-n+1}^{i-1},{w'_{i}})$$\n\n$$\\delta_{n}= \\frac{n_{1}^{n}}{n_{1}^{n} +2n_{2}^{n}}$$\n\n$$n_{1}^{n} = {\\mid}\\left\\{w_{n}:c(w_{1}^{n}) = 1  \\right\\} {\\mid}$$\n$$n_{2}^{n} = {\\mid}\\left\\{w_{n}:c(w_{1}^{n}) = 2  \\right\\} {\\mid}$$\n\nIn other words, $N_{1+}(w_{i-n+1}^{i}{\\bullet})$ is the number of words that **succeed** $w_{i-n+1}^{i}$ at least once in the corpus.\n\n#### 5.2.3. Highest Order Ngrams Equation\n\nAnd finally for highest order ngrams **(n)** the following one:\n\n$$P_{KN}^{n}(w_{i}{\\mid}w_{i-n+1}^{i}) = \n\\frac{max\\left\\{c(w_{i-n+1}^{i}) -{\\delta}_{n},0\\right\\} } \n{ \\sum_{w'_{i}} c(w_{i-n+1}^{i-1},{w'_{i}})} + \\frac{{\\delta}_{n}} {\\sum_{w'_{i}} c(w_{i-n+1}^{i-1},{w'_{i}})} \nN_{1+}(w_{i-n+1}^{i-1}\\bullet )P_{KN}^{n-1}(w_{i}{\\mid}w_{i-n+2}^{i-1})$$\n\n#### 5.2.4. Example\n\nFor example, suppose that we want to calculate the Kneser-ney probability of the quadgram **(in the next future)**.\n\nUsing **5.2.3 equation for highest order (quadgrams)**, $P_{KN}^{4}(future{\\mid}in,the,next)$ is equal to:\n\n$$P_{KN}^{4}(future{\\mid}in,the,next) = \n\\frac{max\\left\\{c(in,the,next,future) -{\\delta}_{4},0\\right\\} } \n{ \\sum_{w'_{i}} c(in,the,next,{w'_{i}})} + $$\n$$\\frac{{\\delta}_{4}} {\\sum_{w'_{i}} c(in,the,next,{w'_{i}})} \nN_{1+}(in,the,next,\\bullet )\nP_{KN}^{3}(future{\\mid}the,next)$$\n\nwhere:\n\n$$\\delta_{4}= \\frac{n_{1}^{4}}{n_{1}^{4} +2n_{2}^{4}}$$\n$$n_{1}^{4} = {\\mid}\\left\\{w_{4}:c(w_{1}^{4}) = 1  \\right\\} {\\mid}$$\n$$n_{2}^{4} = {\\mid}\\left\\{w_{4}:c(w_{1}^{4}) = 2  \\right\\} {\\mid}$$\n\nIn order to calculate $P_{KN}^{3}(future{\\mid}the,next)$ we use the **5.2.2 equation for trigrams**:  \n\n$$P_{KN}^{3}(future{\\mid}the,next) = \n\\frac{max\\left\\{ N_{1+}(\\bullet the,next,future)-{\\delta}_{3},0\\right\\} }\n{ N_{1+}(\\bullet the,next, \\bullet)} + $$\n$$\n\\frac{{\\delta}_{3}} { N_{1+}(\\bullet the,next \\bullet)}\nN_{1+}(the,next \\bullet)P_{KN}^{2}(future{\\mid}next)$$\n\nwhere:\n\n$$\\delta_{3}= \\frac{n_{1}^{3}}{n_{1}^{3} +2n_{2}^{3}}$$\n$$n_{1}^{3} = {\\mid}\\left\\{w_{3}:c(w_{1}^{3}) = 1  \\right\\} {\\mid}$$\n$$n_{2}^{3} = {\\mid}\\left\\{w_{3}:c(w_{1}^{3}) = 2  \\right\\} {\\mid}$$\n\nIn a similar way, to calculate $P_{KN}^{2}(future{\\mid}next)$ we use again the **5.2.2 equation for bigrams**:\n\n$$P_{KN}^{2}(future{\\mid}next) = \n\\frac{max\\left\\{ N_{1+}(\\bullet next,future)-{\\delta}_{2},0\\right\\} }\n{ N_{1+}(\\bullet next \\bullet)} + $$\n$$\n\\frac{{\\delta}_{2}} { N_{1+}(\\bullet next \\bullet)}\nN_{1+}(next,\\bullet)P_{KN}^{1}(future)$$\n\nwhere:\n\n$$\\delta_{2}= \\frac{n_{1}^{2}}{n_{1}^{2} +2n_{2}^{2}}$$\n$$n_{1}^{2} = {\\mid}\\left\\{w_{2}:c(w_{1}^{2}) = 1  \\right\\} {\\mid}$$\n$$n_{2}^{2} = {\\mid}\\left\\{w_{2}:c(w_{1}^{2}) = 2  \\right\\} {\\mid}$$\n\nAnd finally, using **5.2.1 equation for lowest order (unigrams)**, $P_{KN}^{1}(future)$ is equal to:\n\n$$P_{KN}^{1}(future) = \\frac{N_{1+}({\\bullet} future)} { N_{1+}({\\bullet} {\\bullet})}$$\n\nIn the following sections we will describe the different aspects considered to create the Kneser-ney algoritm.\n\n#### 5.2.1. Recursive Kneser-Ney Implementation\n\nWe consider to implement the recursion notation based on the previous section. Because the algoritm was written in a recursive way, at the beggining we decide to implement it in the same way.\nThe running time of the recursive Kneser-ney implementation was almost imposible to finish, at one moment the Bigrams table will take more than 4 days to finish (based on some debugging values provided when was running), so I believe that was an issue related with the processing capacity, so I decided to explore different ways to improve the performace by parallelize. I try different things:\n\n* Use some libraries (like **parallel**)\n* Use a R version version from Microsoft that support multithreading (**R Open**)\n\nBut I didn't get any good results. After that and make a lot of research I found and clear statement that explain that R is not a good platform for a recursive algorithm. So I decided to change the approach and create a non-recursive algorithm.\n\n#### 5.2.2. Non-Recursive Implementation\n\nI start working in the non-recursive (iterative) version, getting a reduction on running time but that even will take more than 7 hours to run for bigrams, so I start to check other alternatives and I found one of my biggest issues:\n\n#### 5.2.2.1. Data Frame vs Data Table\n\nAt the beggining I create an algorithm using **data.frame** objects to store and make the calculations. After several research I found in several blogs different articles that show the big running time difference between **data.table** and **data.frame**, so I decided to use that.\nThe change was exponential, the running time was reduced from hours to minutes.\n\n#### 5.2.2.2. Join vs Add the values needed on the Same Table\n\nIn Kneser-ney algorithm is neccesary to calculate some special values like **N1+(* w)** and also low order probability calculation that involve several ngrams table. \n\nFor example, to calculate a **trigram probability** we need also to calculate the **low order bigram probability**, that also needs the **lowest order probability value (unigram)**, to do that at the beginning I was taking the values **from different tables at the calculation moment**. A big running time difference appears when the calculation is made with all the values on the same table. I mean, add all the values needed to make the calculation from bigram and unigram using **merge** to the trigram table.\n\nOnce again the change was exponential, the running time was reduced from minutes to seconds.\n\n#### 5.2.2.3. RAM Memory Management\n\nIt was very important to manage the amount of memory that the algorithm use, removing from the memory the non-needed objects (using the function **rm()**) and calling the garbage collector after the removing process (function **gc()**). \n\n\n### 6. How to Predict Next Word\n\nOnce we got the ngrams tables with the Kneser-ney calculation for our model (unigrams, bigrams, trigrams and quadgrams), the next step is how to use it to predict next word.\n\nAs we said mention on **1.2 Ngrams Model Selection**, this quagram models give to us the ability to predict the next word based on the last three words that should be our best scenario. The main idea is use the last three words and search in the quadgram table that sequence of words and select the ones with higher probability.\n\nUsing this approach and how many words we got, the algorithm will be:\n\n* If exists **at least three words**, pick up the **last three**, search that sequence in the **quadgrams table** and then choose the one with **higher probability** (based on Kneser-ney)\n\n* Else If exists **at least two words**, pick up the **last two**, search that sequence in the **trigrams table** and then choose the one with **higher probability** (based on Kneser-ney).\n\n* Else If exists **at least one word**, search that word in the **bigrams table** and then choose the one with **higher probability** (based on Kneser-ney).\n\n* Else (if doesnt exists words) choose in the **unigrams table** the one with **higher probability** (based on MLE).\n\nThis works if we found the sequence of words (last three, last two or last one) in the related ngram table (quadgrams, trigrams and bigrams) but, what will happend if we can not find this sequence? this is something possible because we can miss some sequence in our corpora. To handle this we should use a **backoff** strategy.\n\n**Backoff** means that you go back to a n-1 gram level to calculate the probabilities when you encounter a word with prob=0 (doesn't exists). For example we will use a trigram model (that is based only in the last two words) to calculate the probability of a sequence of three words that doesn't exists in the quadgram table.\n\n#### 6.1. A Backoff Strategy\n\nWe will use the **Stupid Backoff** scheme, that states that whenever you go back 1 level you multiply the probabilities by **0.4**. The algorithm will be:\n\n* If the **last three words sequence** doesn't exists in the **quadgrams table** let's **backoff to a trigrams model** considering only the **last two words** and a new probability equal to **0.4 * trigrams prob**.\n\n* If the **last two words sequence** doesn't exists in the **trigrams table** let's **backoff to a bigrams model** considering only the **last words** and a new probability equal to **0.4 * 0.4 * bigrams prob**.\n\n* If the **last word** doesn't exists in the **bigrams table** let's **backoff to a unigrams model** with a new probability equal to ** 0.4 * 0.4 * 0.4 * unigrams prob**.\n\nAs an example, let's consider that we want to predict the next word based on the sequence **\"this is a paper\"** using our **quadgram model**, we will pick up the last three words **\"is a paper\"** and search that sequence in the **trigram table**, suppose that this sequence doesn't exists, so we must **backoff to trigram model**, that means use the sequence **\"a paper\"** and calculate the probability for each **trigram = 0.4 * current trigram probability**. \n\nIf the sequence **\"a paper\"** doesn't exists in the bigram table, we must **backoff to bigram model**, considering only **\"paper\"** and calculate the probability for each **bigram = 0.4 * 0.4 * current bigram probability**. \n\nAgain, if **\"paper\"** doesn't exists in the bigrams table we must **backoff to unigram model** calculating the probability for each **unigram = 0.4 * 0.4 * 0.4 * current unigram probability**. \n\n#### 6.2. Prediction based on Incomplete Word\n\nTo provide some interesting features to the Shiny App we consider to implement a prediction algorithm that consider the last word in the sequence as a **incomplete word**, that means will be used as a **regex** to search words in the highest ngram table.\n\nAs an example, let's consider that the following **trigrams** exists into the **trigrams table**:\n\n* (is a paper)\n* (is a paperless)\n* (is a paperline)\n* (is a paperpop)\n\nLet's consider that we want to predict the next word of the sequence **\"this is a paper\"** considering the last word as **incomplete** using our **quadgram model**. We will pick up the last three words **\"is a paper\"** consider the last word **\"paper\"** as regex to search into the trigrams table.  Our predicting algorithm will provide the previous trigrams as results. \n\nIf the regex search of **\"is a paper\"** in trigrams table didn't provide any results, we should **backoff to bigram model** so we will search **\"a paper\"** considering **\"paper\"** as regex in bigrams table. If we didn't find any match, the final step is **backoff to unigram level** using **\"paper\"** as a regex to search into unigram table.  \n\nThinking on the final Shiny App, using this method we can predict next word every time that the user introduce a key an will be very interactive to the end user. \n\n#### 6.3. Number of Predicted Words\n\nThinking on the final Shiny App, we consider to implement a prediction algorithm that consider as a parameter the **amount of predicted words**, so we will make the prediction and choose the top-n words according with this. With this idea, we will also use the **backoff** strategy if we didn't find enought words to cover the ones needed. At the end we choose the top-n words based on the related probability value.\n\n\n#### 6.4. Minimun Probability of Predicted Words\n\nWith the same objective that the previous idea, we consider to implement a prediction algorithm that consider as a parameter the **minimun probability of predicted words**, so we will make the prediction and choose the top-n words that **have a probability equal or higher than a provided value**. With this idea, we will also use the **backoff** strategy if we didn't find enought words that comply with the requirements. At the end we choose the top-n words based on the related probability value.\n\n\n### 7. How to reduce the size of Probability Ngrams Tables?\n\nTo improve the running speed and produce a suitable final solution, we consider to reduce the size of probability ngrams tables based on the frequency of each ngram.\n\nTo do this we validate in each ngram table (unigrams, bigrams, trigrams and quadgrams) the amount of ngrams that have frequency equal to 1, 2, 3, 4 and 5.\n\nWe got interesting results that shows that a big amounts of ngrams have very low frequency (1, 2, 3 or 4) that could be removed because it will not affect the results (because those ngrams are not so frequent, was mainly part of some typing mistakes). We choose to reduce the size of the ngrams tables considering two types:\n\n* Ngrams with frequency higher than 1 (remove singletons)\n* Ngrams with frequency higher than 4.\n\nThis idea reduce a big the size of each ngram probability table as show in the next code:\n\n### 8. The Shiny App\n\nThe Shiny App is very simple, was named **Predictor!** as has:\n\n* A textarea when the end user will introduce the text to be used to predict next word. This text must will be input on lowercase. The application run every time that the user input a key, so is not neccesary to push a button to get results. \n\n* A panel that have several parameters/options to choose/select, this panel will be described later.\n\n* A label where the top word is selected and show it as a result of the prediction. The text input by the user will be showed in black color and the predicted word (or the portion of that word in case of **incomplete prediction** will be showed in blue color. \n\n* A result table that provide the information of the top-n words (the result of the prediction algorithm)\n\n* A wordcloud of the top-n words. \n\nWe also include several tabpanels with additional interesting information like:\n\n* The Ngrams Probability Tables\n\n* The **Middle End** report\n\n* A Probability Data Table Frequency Analisys\n\n* The \"Final Report\" (this document)\n\n#### 8.1. User Panel Parameters\n\nAs parameters the user could choose/select the following: \n\n* **Ngram Probability Table**: the user could choose to use two types of probability tables: **with Freq >= 5** and **with Freq > 1** in order to evaluate if this affect the prediction results. The default is **Freq >= 5**. \n\n* **Prediction Method**: the user could choose to use two types of prediction method: **Complete Words** and **Incomplete Words**. When **Complete Words** method is selected (default), the application only will show a prediction when the user press the space bar. If the **Incomplete Words** method is selected, after any key input the prediction will be shown. The default is **Incomplete Words**.\n\n* **Probability Range**: the user could choose to minimun probability to consider for the prediction result. The range is between **0** and **1**. The predicted words provided as result must have a probability **higher of equal to value selected**. The default value is **0**.\n\n* **Maximun Number of Words**: the user could choose the amount of words that the prediction algorithm will provide as results. The range is between **1** to **20**. The default value is **5**.\n\n\n\n",
    "created" : 1468788443770.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2721535021",
    "id" : "7FA51120",
    "lastKnownWriteTime" : 1468783243,
    "path" : "D:/001 -- Coursera/Capstone Project/Coursera---Data-Science---Capstone-Project/Final Report.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "type" : "r_markdown"
}